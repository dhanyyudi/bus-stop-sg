name: Bus Stop Enhanced Monitor with Dynamic Downloads

on:
  schedule:
    - cron: "0 1 * * 1" # Every Monday at 1 AM UTC
  workflow_dispatch:
    inputs:
      limit:
        description: "Test limit (optional)"
        required: false
        default: ""
      workers:
        description: "Number of workers"
        required: false
        default: "4"
      log_level:
        description: "Log level (INFO, DEBUG)"
        required: false
        default: "INFO"

jobs:
  collect-and-notify:
    runs-on: ubuntu-latest
    timeout-minutes: 180
    permissions:
      contents: write
    steps:
      - name: Send start notification
        env:
          SLACK_WEBHOOK: ${{ secrets.SLACK_WEBHOOK }}
        run: |
          if [ -n "$SLACK_WEBHOOK" ]; then
            curl -X POST -H 'Content-type: application/json' \
              --data '{"text":"üîÑ Bus Stop Collection Started - Enhanced version with dynamic downloads"}' \
              "$SLACK_WEBHOOK"
          else
            echo "WARNING: SLACK_WEBHOOK not configured"
          fi

      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.10"

      - name: Install dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y chromium-browser chromium-chromedriver jq
          pip install selenium beautifulsoup4 pandas tqdm webdriver-manager requests numpy

      - name: Set environment encoding
        run: |
          echo "PYTHONIOENCODING=utf-8" >> $GITHUB_ENV
          echo "LC_ALL=C.UTF-8" >> $GITHUB_ENV
          echo "LANG=C.UTF-8" >> $GITHUB_ENV

      - name: Run enhanced data collection
        env:
          LTA_API_KEY: ${{ secrets.LTA_API_KEY }}
          PYTHONIOENCODING: utf-8
        run: |
          echo "Starting enhanced bus stop data collection..."
          echo "Using script: bus_stop_merger_final.py"

          python bus_stop_merger_final.py \
            --lta-api-key "$LTA_API_KEY" \
            --workers ${{ github.event.inputs.workers || '4' }} \
            --batch-size 20 \
            --log-level ${{ github.event.inputs.log_level || 'INFO' }} \
            ${{ github.event.inputs.limit && format('--limit {0}', github.event.inputs.limit) || '' }}

          echo "Enhanced data collection completed!"

      - name: Extract enhanced statistics with date comparison
        run: |
          echo "Extracting enhanced statistics with date comparison..."
          python3 << 'EOF'
          import pandas as pd
          import os
          import glob
          import json
          import datetime
          import re

          # Initialize default values
          stats = {
              'TOTAL_STOPS': 0,
              'CORRECTIONS': 0,
              'SUCCESS_RATE': 100.0,
              'CORRECTION_PCT': 0.0,
              'CHANGES_DETECTED': 0,
              'NEW_STOPS': 0,
              'NAME_CHANGES': 0,
              'REMOVED_STOPS': 0,
              'EFFICIENCY': 100.0,
              'PREVIOUS_DATE': 'Unknown',
              'CURRENT_DATE': 'Unknown',
              'NEW_STOPS_DETAILS': '[]'
          }

          current_date = datetime.datetime.now().strftime("%d%m%Y")
          stats['CURRENT_DATE'] = datetime.datetime.now().strftime("%d/%m/%Y")

          try:
              # Load main result file
              if os.path.exists('data/lta_correction.csv'):
                  df = pd.read_csv('data/lta_correction.csv')
                  stats['TOTAL_STOPS'] = len(df)
                  
                  if 'name_source' in df.columns:
                      stats['CORRECTIONS'] = len(df[df['name_source'] == 'SimplyGo'])
                      stats['CORRECTION_PCT'] = (stats['CORRECTIONS'] / stats['TOTAL_STOPS'] * 100) if stats['TOTAL_STOPS'] > 0 else 0
                  
                  print(f"‚úÖ Loaded main results: {stats['TOTAL_STOPS']} stops, {stats['CORRECTIONS']} corrections")
              
              # Find previous LTA file to get comparison date
              lta_files = glob.glob('data/LTA_bus_stops_*.csv')
              if lta_files:
                  # Filter out current date file and get the most recent previous file
                  previous_files = []
                  for file in lta_files:
                      filename = os.path.basename(file)
                      date_match = re.search(r'LTA_bus_stops_(\d{8})\.csv', filename)
                      if date_match and date_match.group(1) != current_date:
                          file_date = date_match.group(1)
                          # Convert DDMMYYYY to datetime for sorting
                          try:
                              dt = datetime.datetime.strptime(file_date, "%d%m%Y")
                              previous_files.append((file, dt, file_date))
                          except:
                              pass
                  
                  if previous_files:
                      # Sort by date and get the most recent
                      previous_files.sort(key=lambda x: x[1], reverse=True)
                      latest_previous = previous_files[0]
                      prev_date_str = latest_previous[2]  # DDMMYYYY format
                      
                      # Convert to readable format DD/MM/YYYY
                      try:
                          dt = datetime.datetime.strptime(prev_date_str, "%d%m%Y")
                          stats['PREVIOUS_DATE'] = dt.strftime("%d/%m/%Y")
                      except:
                          stats['PREVIOUS_DATE'] = prev_date_str
                      
                      print(f"üìÖ Previous data date: {stats['PREVIOUS_DATE']}")
              
              # Try to get change statistics and details
              change_files = glob.glob('data/LTA_changes_*.csv')
              if change_files:
                  latest_change_file = sorted(change_files)[-1]
                  print(f"üìä Loading changes from: {latest_change_file}")
                  
                  changes_df = pd.read_csv(latest_change_file)
                  stats['CHANGES_DETECTED'] = len(changes_df)
                  
                  if 'change_type' in changes_df.columns:
                      new_stops_df = changes_df[changes_df['change_type'] == 'new']
                      stats['NEW_STOPS'] = len(new_stops_df)
                      stats['NAME_CHANGES'] = len(changes_df[changes_df['change_type'] == 'name_changed'])
                      stats['REMOVED_STOPS'] = len(changes_df[changes_df['change_type'] == 'removed'])
                      
                      # Get details of new stops for display
                      if not new_stops_df.empty and len(new_stops_df) <= 10:  # Only if reasonable number
                          new_stops_details = []
                          for _, row in new_stops_df.iterrows():
                              detail = {
                                  'code': str(row.get('code', '')),
                                  'name': str(row.get('name', '')),
                                  'street': str(row.get('street', ''))
                              }
                              new_stops_details.append(detail)
                          
                          stats['NEW_STOPS_DETAILS'] = json.dumps(new_stops_details)
                          print(f"üìù New stops details: {len(new_stops_details)} stops captured")
                  
                  print(f"üìà Changes: {stats['CHANGES_DETECTED']} total ({stats['NEW_STOPS']} new, {stats['NAME_CHANGES']} renamed, {stats['REMOVED_STOPS']} removed)")
              
              # Calculate efficiency
              if stats['CHANGES_DETECTED'] > 0 and stats['TOTAL_STOPS'] > 0:
                  stats['EFFICIENCY'] = ((stats['TOTAL_STOPS'] - stats['CHANGES_DETECTED']) / stats['TOTAL_STOPS'] * 100)
              
              print("üìä Final Enhanced Statistics:")
              for key, value in stats.items():
                  if key != 'NEW_STOPS_DETAILS':  # Skip long JSON in logs
                      print(f"   {key}: {value}")
              
          except Exception as e:
              print(f"‚ùå Error extracting enhanced stats: {e}")
              # Keep default values

          # Write to environment file
          with open(os.environ['GITHUB_ENV'], 'a') as f:
              for key, value in stats.items():
                  f.write(f"{key}={value}\n")

          print("‚úÖ Enhanced statistics extraction completed")
          EOF

      - name: Create user-friendly summary files
        run: |
          echo "Creating user-friendly summary files for download..."
          python3 << 'EOF'
          import pandas as pd
          import os
          import glob
          import datetime
          import json

          try:
              current_date = datetime.datetime.now().strftime("%d%m%Y")
              readable_date = datetime.datetime.now().strftime("%d_%m_%Y")
              
              # Find latest changes file
              change_files = glob.glob('data/LTA_changes_*.csv')
              if not change_files:
                  print("‚ùå No changes file found")
                  exit(0)
              
              latest_change_file = sorted(change_files)[-1]
              print(f"üìä Processing changes from: {latest_change_file}")
              
              # Load changes data
              changes_df = pd.read_csv(latest_change_file)
              
              if changes_df.empty:
                  print("‚ÑπÔ∏è No changes detected, skipping summary creation")
                  exit(0)
              
              # Create summary by change type
              summary_stats = {}
              if 'change_type' in changes_df.columns:
                  summary_stats = changes_df['change_type'].value_counts().to_dict()
              
              # ============ 1. CREATE NEW STOPS ONLY FILE ============
              if 'change_type' in changes_df.columns:
                  new_stops = changes_df[changes_df['change_type'] == 'new'].copy()
                  
                  if not new_stops.empty:
                      # Select and rename columns for better readability
                      if 'corrected_name' in new_stops.columns:
                          new_stops_clean = new_stops[['code', 'corrected_name', 'street', 'lat', 'lon']].copy()
                          new_stops_clean = new_stops_clean.rename(columns={
                              'code': 'Bus Stop Code',
                              'corrected_name': 'Bus Stop Name',
                              'street': 'Street/Road',
                              'lat': 'Latitude',
                              'lon': 'Longitude'
                          })
                      else:
                          new_stops_clean = new_stops[['code', 'name', 'street', 'lat', 'lon']].copy()
                          new_stops_clean = new_stops_clean.rename(columns={
                              'code': 'Bus Stop Code',
                              'name': 'Bus Stop Name', 
                              'street': 'Street/Road',
                              'lat': 'Latitude',
                              'lon': 'Longitude'
                          })
                      
                      # Add metadata columns
                      new_stops_clean.insert(0, 'Change Type', 'NEW')
                      new_stops_clean.insert(1, 'Date Added', readable_date.replace('_', '/'))
                      
                      # Save new stops only file
                      new_stops_file = f"data/new_bus_stops_{readable_date}.csv"
                      new_stops_clean.to_csv(new_stops_file, index=False)
                      
                      print(f"‚úÖ Created new stops file: {new_stops_file} ({len(new_stops_clean)} stops)")
              
              # ============ 2. CREATE NAME CHANGES ONLY FILE ============
              if 'change_type' in changes_df.columns:
                  name_changes = changes_df[changes_df['change_type'] == 'name_changed'].copy()
                  
                  if not name_changes.empty:
                      # Create name changes summary
                      name_changes_clean = name_changes[['code', 'name', 'old_name', 'street']].copy() if 'old_name' in name_changes.columns else name_changes[['code', 'name', 'street']].copy()
                      name_changes_clean = name_changes_clean.rename(columns={
                          'code': 'Bus Stop Code',
                          'name': 'New Name',
                          'old_name': 'Previous Name',
                          'street': 'Street/Road'
                      })
                      
                      # Add metadata
                      name_changes_clean.insert(0, 'Change Type', 'NAME_CHANGED')
                      name_changes_clean.insert(1, 'Date Changed', readable_date.replace('_', '/'))
                      
                      # Save name changes file
                      name_changes_file = f"data/name_changes_{readable_date}.csv"
                      name_changes_clean.to_csv(name_changes_file, index=False)
                      
                      print(f"‚úÖ Created name changes file: {name_changes_file} ({len(name_changes_clean)} changes)")
              
              # ============ 3. CREATE COMPREHENSIVE SUMMARY FILE ============
              # This will be the main "changes only" download
              summary_data = []
              
              # Process each change type
              for change_type in ['new', 'name_changed', 'removed']:
                  if 'change_type' in changes_df.columns:
                      subset = changes_df[changes_df['change_type'] == change_type].copy()
                      
                      if not subset.empty:
                          for _, row in subset.iterrows():
                              summary_row = {
                                  'Change Type': change_type.upper().replace('_', ' '),
                                  'Bus Stop Code': row.get('code', ''),
                                  'Bus Stop Name': row.get('corrected_name', row.get('name', '')),
                                  'Street/Road': row.get('street', ''),
                                  'Latitude': row.get('lat', ''),
                                  'Longitude': row.get('lon', ''),
                                  'Date Processed': readable_date.replace('_', '/'),
                                  'Source': row.get('name_source', 'LTA'),
                              }
                              
                              # Add old name for name changes
                              if change_type == 'name_changed' and 'old_name' in row:
                                  summary_row['Previous Name'] = row.get('old_name', '')
                              
                              # Add change reason if available
                              if 'change_reason' in row:
                                  summary_row['Reason'] = row.get('change_reason', '')
                              
                              summary_data.append(summary_row)
              
              if summary_data:
                  summary_df = pd.DataFrame(summary_data)
                  
                  # Save comprehensive summary
                  summary_file = f"data/bus_stop_changes_summary_{readable_date}.csv"
                  summary_df.to_csv(summary_file, index=False)
                  
                  print(f"‚úÖ Created comprehensive summary: {summary_file} ({len(summary_df)} total changes)")
                  
                  # Create a simple text summary too
                  text_summary = f"""Bus Stop Changes Summary - {readable_date.replace('_', '/')}
          ===============================================

          Total Changes Detected: {len(summary_df)}

          Breakdown by Change Type:
          """
                  
                  for change_type, count in summary_stats.items():
                      text_summary += f"  ‚Ä¢ {change_type.upper().replace('_', ' ')}: {count}\n"
                  
                  text_summary += f"""
          Files Generated:
          ‚Ä¢ bus_stop_changes_summary_{readable_date}.csv - All changes in one file  
          ‚Ä¢ new_bus_stops_{readable_date}.csv - New stops only
          ‚Ä¢ name_changes_{readable_date}.csv - Name changes only

          Data Source: LTA DataMall + SimplyGo corrections
          Generated: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
          """
                  
                  # Save text summary
                  text_summary_file = f"data/changes_summary_{readable_date}.txt"
                  with open(text_summary_file, 'w', encoding='utf-8') as f:
                      f.write(text_summary)
                  
                  print(f"‚úÖ Created text summary: {text_summary_file}")
              
              print("‚úÖ All user-friendly summary files created successfully!")
              
          except Exception as e:
              print(f"‚ùå Error creating summary files: {e}")
              import traceback
              traceback.print_exc()
          EOF

      - name: Check for significant changes
        run: |
          echo "Checking for significant changes..."

          SIGNIFICANT_CHANGE=false

          if [ "${CHANGES_DETECTED:-0}" -gt 50 ]; then
            echo "SIGNIFICANT: Large number of changes detected ($CHANGES_DETECTED)"
            SIGNIFICANT_CHANGE=true
          fi

          if [ "${NEW_STOPS:-0}" -gt 20 ]; then
            echo "SIGNIFICANT: Many new bus stops ($NEW_STOPS)"
            SIGNIFICANT_CHANGE=true
          fi

          if [ "${REMOVED_STOPS:-0}" -gt 20 ]; then
            echo "SIGNIFICANT: Many removed bus stops ($REMOVED_STOPS)"
            SIGNIFICANT_CHANGE=true
          fi

          echo "SIGNIFICANT_CHANGE=$SIGNIFICANT_CHANGE" >> $GITHUB_ENV

      - name: Commit results with summary files
        run: |
          echo "Committing results and summary files to repository..."
          git config --global user.name 'GitHub Actions'
          git config --global user.email 'actions@github.com'

          TIMESTAMP=$(date +'%d%m%Y_%H%M')
          readable_date=$(date +'%d/%m/%Y')

          # Add main data files
          [ -f "data/lta_correction.csv" ] && git add "data/lta_correction.csv"

          # Add all data files
          for file in data/*.csv data/*.txt logs/*.log output/*.csv; do
            [ -f "$file" ] && git add "$file"
          done

          # Count summary files for commit message
          SUMMARY_FILES_COUNT=0
          [ -f "data/new_bus_stops_"*".csv" ] && SUMMARY_FILES_COUNT=$((SUMMARY_FILES_COUNT + 1))
          [ -f "data/name_changes_"*".csv" ] && SUMMARY_FILES_COUNT=$((SUMMARY_FILES_COUNT + 1))  
          [ -f "data/bus_stop_changes_summary_"*".csv" ] && SUMMARY_FILES_COUNT=$((SUMMARY_FILES_COUNT + 1))
          [ -f "data/changes_summary_"*".txt" ] && SUMMARY_FILES_COUNT=$((SUMMARY_FILES_COUNT + 1))

          # Create comprehensive commit message
          COMMIT_MSG="üöå Bus data update ${readable_date}: ${TOTAL_STOPS:-0} stops"

          if [ "${CHANGES_DETECTED:-0}" -gt 0 ]; then
            COMMIT_MSG="$COMMIT_MSG, ${CHANGES_DETECTED} changes detected"
            
            # Add change breakdown
            [ "${NEW_STOPS:-0}" -gt 0 ] && COMMIT_MSG="$COMMIT_MSG (+${NEW_STOPS} new)"
            [ "${NAME_CHANGES:-0}" -gt 0 ] && COMMIT_MSG="$COMMIT_MSG (~${NAME_CHANGES} renamed)"
            [ "${REMOVED_STOPS:-0}" -gt 0 ] && COMMIT_MSG="$COMMIT_MSG (-${REMOVED_STOPS} removed)"
            
            # Add summary files info
            if [ "$SUMMARY_FILES_COUNT" -gt 0 ]; then
              COMMIT_MSG="$COMMIT_MSG, ${SUMMARY_FILES_COUNT} summary files"
            fi
          else
            COMMIT_MSG="$COMMIT_MSG, no changes detected"
          fi

          COMMIT_MSG="$COMMIT_MSG, ${CORRECTIONS:-0} corrections applied"

          # Add efficiency info
          if [ "${CHANGES_DETECTED:-0}" -gt 0 ]; then
            COMMIT_MSG="$COMMIT_MSG (${EFFICIENCY:-100}% efficiency)"
          fi

          # Commit with descriptive message
          if git diff --staged --quiet; then
            echo "‚ÑπÔ∏è No changes to commit"
          else
            git commit -m "$COMMIT_MSG"
            echo "‚úÖ Committed with message: $COMMIT_MSG"
            
            # Show what was committed
            echo "üìÅ Files committed:"
            git diff --name-only HEAD~1 HEAD | sed 's/^/   ‚Ä¢ /'
          fi

          # Push with retry mechanism
          echo "‚¨ÜÔ∏è Pushing to remote repository..."
          if ! git push origin main; then
            echo "‚ö†Ô∏è First push failed, trying pull and rebase..."
            git pull --rebase origin main
            git push origin main
          fi

          echo "‚úÖ Successfully pushed to repository"

      - name: Create dynamic download buttons for Slack
        run: |
          echo "Creating dynamic download buttons based on detected changes..."

          # Base repository URL
          REPO_URL="https://github.com/${GITHUB_REPOSITORY}/raw/main/data"

          # Current date for file names
          READABLE_DATE=$(date +'%d_%m_%Y')

          # Initialize button arrays
          DOWNLOAD_BUTTONS=""
          AVAILABLE_FILES=""

          # Check for main changes file
          if [ -f "$(ls -t data/LTA_changes_*.csv 2>/dev/null | head -1)" ]; then
            CHANGES_FILE=$(basename "$(ls -t data/LTA_changes_*.csv | head -1)")
            DOWNLOAD_BUTTONS="${DOWNLOAD_BUTTONS}
                    {
                      \"type\": \"button\",
                      \"text\": \"üì• All Changes (${CHANGES_DETECTED:-0})\",
                      \"url\": \"${REPO_URL}/${CHANGES_FILE}\",
                      \"style\": \"primary\"
                    },"
            AVAILABLE_FILES="${AVAILABLE_FILES}‚Ä¢ **All Changes**: Complete change log with technical details\\n"
          fi

          # Check for new stops summary file
          if [ "${NEW_STOPS:-0}" -gt 0 ] && [ -f "data/new_bus_stops_${READABLE_DATE}.csv" ]; then
            DOWNLOAD_BUTTONS="${DOWNLOAD_BUTTONS}
                    {
                      \"type\": \"button\",
                      \"text\": \"üÜï New Stops (${NEW_STOPS})\",
                      \"url\": \"${REPO_URL}/new_bus_stops_${READABLE_DATE}.csv\",
                      \"style\": \"good\"
                    },"
            AVAILABLE_FILES="${AVAILABLE_FILES}‚Ä¢ **New Stops Only**: ${NEW_STOPS} newly added bus stops\\n"
          fi

          # Check for name changes file
          if [ "${NAME_CHANGES:-0}" -gt 0 ] && [ -f "data/name_changes_${READABLE_DATE}.csv" ]; then
            DOWNLOAD_BUTTONS="${DOWNLOAD_BUTTONS}
                    {
                      \"type\": \"button\",
                      \"text\": \"‚úèÔ∏è Name Changes (${NAME_CHANGES})\",
                      \"url\": \"${REPO_URL}/name_changes_${READABLE_DATE}.csv\",
                      \"style\": \"default\"
                    },"
            AVAILABLE_FILES="${AVAILABLE_FILES}‚Ä¢ **Name Changes**: Stops with updated names\\n"
          fi

          # Check for comprehensive summary
          if [ -f "data/bus_stop_changes_summary_${READABLE_DATE}.csv" ]; then
            DOWNLOAD_BUTTONS="${DOWNLOAD_BUTTONS}
                    {
                      \"type\": \"button\",
                      \"text\": \"üìä Summary Report\",
                      \"url\": \"${REPO_URL}/bus_stop_changes_summary_${READABLE_DATE}.csv\",
                      \"style\": \"default\"
                    },"
            AVAILABLE_FILES="${AVAILABLE_FILES}‚Ä¢ **Summary Report**: User-friendly consolidated changes\\n"
          fi

          # Always include complete dataset
          DOWNLOAD_BUTTONS="${DOWNLOAD_BUTTONS}
                  {
                    \"type\": \"button\",
                    \"text\": \"üìä Complete Dataset (${TOTAL_STOPS:-0})\",
                    \"url\": \"${REPO_URL}/lta_correction.csv\",
                    \"style\": \"default\"
                  },"
          AVAILABLE_FILES="${AVAILABLE_FILES}‚Ä¢ **Complete Dataset**: All bus stops with corrections\\n"

          # Add GitHub and logs buttons
          DOWNLOAD_BUTTONS="${DOWNLOAD_BUTTONS}
                  {
                    \"type\": \"button\",
                    \"text\": \"üìà View Repository\",
                    \"url\": \"https://github.com/${GITHUB_REPOSITORY}\",
                    \"style\": \"default\"
                  },
                  {
                    \"type\": \"button\",
                    \"text\": \"üîç View Logs\",
                    \"url\": \"https://github.com/${GITHUB_REPOSITORY}/actions\",
                    \"style\": \"default\"
                  }"

          # Remove trailing comma
          DOWNLOAD_BUTTONS=$(echo "$DOWNLOAD_BUTTONS" | sed 's/,$//')
          AVAILABLE_FILES=$(echo "$AVAILABLE_FILES" | sed 's/\\n$//')

          # Save to environment for use in next step
          echo "DOWNLOAD_BUTTONS<<EOF" >> $GITHUB_ENV
          echo "$DOWNLOAD_BUTTONS" >> $GITHUB_ENV
          echo "EOF" >> $GITHUB_ENV

          echo "AVAILABLE_FILES<<EOF" >> $GITHUB_ENV
          echo "$AVAILABLE_FILES" >> $GITHUB_ENV
          echo "EOF" >> $GITHUB_ENV

          echo "‚úÖ Dynamic download buttons created based on available changes"
          echo "üìÅ Available download options:"
          echo "$AVAILABLE_FILES" | sed 's/\\n/\n/g'

      - name: Prepare Slack notification details
        run: |
          # Parse new stops details for display
          NEW_STOPS_LIST=""
          if [ "${NEW_STOPS:-0}" -gt 0 ] && [ "${NEW_STOPS:-0}" -le 10 ]; then
            # Create a readable list of new stops
            python3 << 'EOF'
          import json
          import os

          try:
              details_json = os.getenv('NEW_STOPS_DETAILS', '[]')
              details = json.loads(details_json)
              
              if details:
                  stops_text = []
                  for stop in details[:5]:  # Limit to first 5 for Slack display
                      code = stop.get('code', 'N/A')
                      name = stop.get('name', 'N/A')
                      street = stop.get('street', 'N/A')
                      stops_text.append(f"‚Ä¢ `{code}` - {name} ({street})")
                  
                  result = "\\n".join(stops_text)
                  if len(details) > 5:
                      result += f"\\n‚Ä¢ ... and {len(details) - 5} more new stops"
                  
                  # Write to file for shell to read
                  with open('new_stops_list.txt', 'w') as f:
                      f.write(result)
                  print("‚úÖ New stops list created")
              else:
                  with open('new_stops_list.txt', 'w') as f:
                      f.write("No details available")
                  print("‚ÑπÔ∏è No new stops details to display")
                  
          except Exception as e:
              print(f"‚ùå Error creating new stops list: {e}")
              with open('new_stops_list.txt', 'w') as f:
                  f.write("Error loading details")
          EOF
              
              NEW_STOPS_LIST=$(cat new_stops_list.txt 2>/dev/null || echo "Details not available")
          fi

          # Determine status color and emoji
          if [ "${CHANGES_DETECTED:-0}" -eq 0 ]; then
            STATUS_COLOR="good"
            STATUS_EMOJI="‚úÖ"
            STATUS_TEXT="No Changes"
          elif [ "${CHANGES_DETECTED:-0}" -le 20 ]; then
            STATUS_COLOR="warning"
            STATUS_EMOJI="‚ö†Ô∏è"
            STATUS_TEXT="Minor Changes"
          else
            STATUS_COLOR="danger"
            STATUS_EMOJI="üö®"
            STATUS_TEXT="Major Changes"
          fi

          # Save to environment
          echo "NEW_STOPS_LIST<<EOF" >> $GITHUB_ENV
          echo "$NEW_STOPS_LIST" >> $GITHUB_ENV
          echo "EOF" >> $GITHUB_ENV

          echo "STATUS_COLOR=$STATUS_COLOR" >> $GITHUB_ENV
          echo "STATUS_EMOJI=$STATUS_EMOJI" >> $GITHUB_ENV
          echo "STATUS_TEXT=$STATUS_TEXT" >> $GITHUB_ENV

      - name: Create final enhanced Slack payload with dynamic buttons
        run: |
          # Create the final enhanced payload with dynamic buttons
          cat > slack_payload.json << EOF
          {
            "text": "üöå Bus Stop Collection Complete",
            "attachments": [
              {
                "color": "${STATUS_COLOR}",
                "title": "${STATUS_EMOJI} Collection Results - ${STATUS_TEXT}",
                "fields": [
                  {
                    "title": "üìä Summary",
                    "value": "Compared *${PREVIOUS_DATE}* ‚Üí *${CURRENT_DATE}*\\nProcessed *${TOTAL_STOPS:-0}* total bus stops",
                    "short": false
                  },
                  {
                    "title": "üìà Changes Detected",
                    "value": "${CHANGES_DETECTED:-0}",
                    "short": true
                  },
                  {
                    "title": "üÜï New Stops",
                    "value": "${NEW_STOPS:-0}",
                    "short": true
                  },
                  {
                    "title": "‚úèÔ∏è Name Changes",  
                    "value": "${NAME_CHANGES:-0}",
                    "short": true
                  },
                  {
                    "title": "üóëÔ∏è Removed Stops",
                    "value": "${REMOVED_STOPS:-0}",
                    "short": true
                  },
                  {
                    "title": "üîÑ Corrections Applied",
                    "value": "${CORRECTIONS:-0} (${CORRECTION_PCT:-0}%)",
                    "short": true
                  },
                  {
                    "title": "‚ö° Efficiency",
                    "value": "${EFFICIENCY:-100}%",
                    "short": true
                  }
                ],
                "actions": [
                  ${DOWNLOAD_BUTTONS}
                ],
                "footer": "Bus Stop SG Collection | Auto-updated weekly",
                "ts": $(date +%s)
              }$(if [ "${NEW_STOPS:-0}" -gt 0 ] && [ -n "$NEW_STOPS_LIST" ] && [ "$NEW_STOPS_LIST" != "Details not available" ]; then echo ",
              {
                \"color\": \"#36a64f\",
                \"title\": \"üÜï New Bus Stops Details\",
                \"text\": \"$NEW_STOPS_LIST\",
                \"footer\": \"Click 'New Stops (${NEW_STOPS:-0})' button above to download CSV with these stops\"
              }"; fi)$(if [ "${CHANGES_DETECTED:-0}" -gt 0 ]; then echo ",
              {
                \"color\": \"#3498db\",
                \"title\": \"üì• Available Downloads\",
                \"text\": \"${AVAILABLE_FILES}\",
                \"footer\": \"All files are automatically generated and ready for download\"
              }"; fi)
            ]
          }
          EOF

          echo "‚úÖ Final enhanced Slack payload created with dynamic download buttons"
          echo "üîç Payload preview:"
          cat slack_payload.json | jq '.' 2>/dev/null || echo "JSON created (jq not available for formatting)"

      - name: Send success notification
        if: success()
        env:
          SLACK_WEBHOOK: ${{ secrets.SLACK_WEBHOOK }}
        run: |
          if [ -n "$SLACK_WEBHOOK" ]; then
            echo "Sending enhanced success notification to Slack..."
            curl -X POST \
              -H 'Content-type: application/json' \
              -d @slack_payload.json \
              "$SLACK_WEBHOOK"
            echo "Enhanced notification sent!"
          else
            echo "SLACK_WEBHOOK not configured, skipping notification"
          fi

      - name: Send failure notification
        if: failure()
        env:
          SLACK_WEBHOOK: ${{ secrets.SLACK_WEBHOOK }}
        run: |
          if [ -n "$SLACK_WEBHOOK" ]; then
            curl -X POST -H 'Content-type: application/json' \
              --data '{
                "text":"‚ùå Bus Stop Collection Failed",
                "attachments":[{
                  "color":"danger",
                  "title":"üö® Enhanced Workflow Error",
                  "text":"The enhanced bus stop collection workflow has failed. Please check the logs for details.",
                  "actions":[{
                    "type":"button",
                    "text":"üîç Check Logs",
                    "url":"https://github.com/'${GITHUB_REPOSITORY}'/actions",
                    "style":"danger"
                  }]
                }]
              }' \
              "$SLACK_WEBHOOK"
          fi

      - name: Summary
        if: always()
        run: |
          echo "================================="
          echo "ENHANCED WORKFLOW COMPLETE"
          echo "================================="
          echo "Total Stops: ${TOTAL_STOPS:-0}"
          echo "Changes Detected: ${CHANGES_DETECTED:-0}"
          echo "  - New: ${NEW_STOPS:-0}"
          echo "  - Renamed: ${NAME_CHANGES:-0}"
          echo "  - Removed: ${REMOVED_STOPS:-0}"
          echo "Corrections: ${CORRECTIONS:-0} (${CORRECTION_PCT:-0}%)"
          echo "Efficiency: ${EFFICIENCY:-100}%"
          echo "Date Range: ${PREVIOUS_DATE} ‚Üí ${CURRENT_DATE}"
          echo "Summary Files Generated: Yes"
          echo "Dynamic Downloads: Available"
          echo "Mode: Enhanced with Smart Downloads"
          echo "================================="
