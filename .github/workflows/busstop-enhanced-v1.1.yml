name: Bus Stop Enhanced Monitor with Dynamic Downloads

on:
  schedule:
    - cron: "0 1 * * 1" # Every Monday at 1 AM UTC
  workflow_dispatch:
    inputs:
      limit:
        description: "Test limit (optional)"
        required: false
        default: ""
      workers:
        description: "Number of workers"
        required: false
        default: "4"
      log_level:
        description: "Log level (INFO, DEBUG)"
        required: false
        default: "INFO"

jobs:
  collect-and-notify:
    runs-on: ubuntu-latest
    timeout-minutes: 180
    permissions:
      contents: write
    steps:
      - name: Send start notification
        env:
          SLACK_WEBHOOK: ${{ secrets.SLACK_WEBHOOK }}
        run: |
          if [ -n "$SLACK_WEBHOOK" ]; then
            curl -X POST -H 'Content-type: application/json' \
              --data '{"text":"üîÑ Bus Stop Collection Started - Enhanced version with dynamic downloads"}' \
              "$SLACK_WEBHOOK"
          else
            echo "WARNING: SLACK_WEBHOOK not configured"
          fi

      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.10"

      - name: Install dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y chromium-browser chromium-chromedriver jq
          pip install selenium beautifulsoup4 pandas tqdm webdriver-manager requests numpy

      - name: Set environment encoding
        run: |
          echo "PYTHONIOENCODING=utf-8" >> $GITHUB_ENV
          echo "LC_ALL=C.UTF-8" >> $GITHUB_ENV
          echo "LANG=C.UTF-8" >> $GITHUB_ENV

      - name: Run enhanced data collection
        env:
          LTA_API_KEY: ${{ secrets.LTA_API_KEY }}
          PYTHONIOENCODING: utf-8
        run: |
          echo "Starting enhanced bus stop data collection..."
          echo "Using script: bus_stop_merger_final.py"

          python bus_stop_merger_final.py \
            --lta-api-key "$LTA_API_KEY" \
            --workers ${{ github.event.inputs.workers || '4' }} \
            --batch-size 20 \
            --log-level ${{ github.event.inputs.log_level || 'INFO' }} \
            ${{ github.event.inputs.limit && format('--limit {0}', github.event.inputs.limit) || '' }}

          echo "Enhanced data collection completed!"

      - name: Extract enhanced statistics with date comparison
        run: |
          echo "Extracting enhanced statistics with date comparison..."
          python3 << 'EOF'
          import pandas as pd
          import os
          import glob
          import json
          import datetime
          import re

          # Initialize default values
          stats = {
              'TOTAL_STOPS': 0,
              'CORRECTIONS': 0,
              'SUCCESS_RATE': 100.0,
              'CORRECTION_PCT': 0.0,
              'CHANGES_DETECTED': 0,
              'NEW_STOPS': 0,
              'NAME_CHANGES': 0,
              'REMOVED_STOPS': 0,
              'EFFICIENCY': 100.0,
              'PREVIOUS_DATE': 'Unknown',
              'CURRENT_DATE': 'Unknown',
              'NEW_STOPS_DETAILS': '[]'
          }

          current_date = datetime.datetime.now().strftime("%d%m%Y")
          stats['CURRENT_DATE'] = datetime.datetime.now().strftime("%d/%m/%Y")

          try:
              # Load main result file
              if os.path.exists('data/lta_correction.csv'):
                  df = pd.read_csv('data/lta_correction.csv')
                  stats['TOTAL_STOPS'] = len(df)
                  
                  if 'name_source' in df.columns:
                      stats['CORRECTIONS'] = len(df[df['name_source'] == 'SimplyGo'])
                      stats['CORRECTION_PCT'] = (stats['CORRECTIONS'] / stats['TOTAL_STOPS'] * 100) if stats['TOTAL_STOPS'] > 0 else 0
                  
                  print(f"‚úÖ Loaded main results: {stats['TOTAL_STOPS']} stops, {stats['CORRECTIONS']} corrections")
              
              # Find previous LTA file to get comparison date
              lta_files = glob.glob('data/LTA_bus_stops_*.csv')
              if lta_files:
                  # Filter out current date file and get the most recent previous file
                  previous_files = []
                  for file in lta_files:
                      filename = os.path.basename(file)
                      date_match = re.search(r'LTA_bus_stops_(\d{8})\.csv', filename)
                      if date_match and date_match.group(1) != current_date:
                          file_date = date_match.group(1)
                          # Convert DDMMYYYY to datetime for sorting
                          try:
                              dt = datetime.datetime.strptime(file_date, "%d%m%Y")
                              previous_files.append((file, dt, file_date))
                          except:
                              pass
                  
                  if previous_files:
                      # Sort by date and get the most recent
                      previous_files.sort(key=lambda x: x[1], reverse=True)
                      latest_previous = previous_files[0]
                      prev_date_str = latest_previous[2]  # DDMMYYYY format
                      
                      # Convert to readable format DD/MM/YYYY
                      try:
                          dt = datetime.datetime.strptime(prev_date_str, "%d%m%Y")
                          stats['PREVIOUS_DATE'] = dt.strftime("%d/%m/%Y")
                      except:
                          stats['PREVIOUS_DATE'] = prev_date_str
                      
                      print(f"üìÖ Previous data date: {stats['PREVIOUS_DATE']}")
              
              # Try to get change statistics and details
              change_files = glob.glob('data/LTA_changes_*.csv')
              if change_files:
                  latest_change_file = sorted(change_files)[-1]
                  print(f"üìä Loading changes from: {latest_change_file}")
                  
                  changes_df = pd.read_csv(latest_change_file)
                  stats['CHANGES_DETECTED'] = len(changes_df)
                  
                  if 'change_type' in changes_df.columns:
                      new_stops_df = changes_df[changes_df['change_type'] == 'new']
                      stats['NEW_STOPS'] = len(new_stops_df)
                      stats['NAME_CHANGES'] = len(changes_df[changes_df['change_type'] == 'name_changed'])
                      stats['REMOVED_STOPS'] = len(changes_df[changes_df['change_type'] == 'removed'])
                      
                      # Get details of new stops for display
                      if not new_stops_df.empty and len(new_stops_df) <= 10:  # Only if reasonable number
                          new_stops_details = []
                          for _, row in new_stops_df.iterrows():
                              detail = {
                                  'code': str(row.get('code', '')),
                                  'name': str(row.get('name', '')),
                                  'street': str(row.get('street', ''))
                              }
                              new_stops_details.append(detail)
                          
                          stats['NEW_STOPS_DETAILS'] = json.dumps(new_stops_details)
                          print(f"üìù New stops details: {len(new_stops_details)} stops captured")
                  
                  print(f"üìà Changes: {stats['CHANGES_DETECTED']} total ({stats['NEW_STOPS']} new, {stats['NAME_CHANGES']} renamed, {stats['REMOVED_STOPS']} removed)")
              
              # Calculate efficiency
              if stats['CHANGES_DETECTED'] > 0 and stats['TOTAL_STOPS'] > 0:
                  stats['EFFICIENCY'] = ((stats['TOTAL_STOPS'] - stats['CHANGES_DETECTED']) / stats['TOTAL_STOPS'] * 100)
              
              print("üìä Final Enhanced Statistics:")
              for key, value in stats.items():
                  if key != 'NEW_STOPS_DETAILS':  # Skip long JSON in logs
                      print(f"   {key}: {value}")
              
          except Exception as e:
              print(f"‚ùå Error extracting enhanced stats: {e}")
              # Keep default values

          # Write to environment file
          with open(os.environ['GITHUB_ENV'], 'a') as f:
              for key, value in stats.items():
                  f.write(f"{key}={value}\n")

          print("‚úÖ Enhanced statistics extraction completed")
          EOF

      - name: Create user-friendly summary files
        run: |
          echo "Creating user-friendly summary files for download..."
          python3 << 'EOF'
          import pandas as pd
          import os
          import glob
          import datetime
          import json

          try:
              current_date = datetime.datetime.now().strftime("%d%m%Y")
              readable_date = datetime.datetime.now().strftime("%d_%m_%Y")
              
              # Find latest changes file
              change_files = glob.glob('data/LTA_changes_*.csv')
              if not change_files:
                  print("‚ùå No changes file found")
                  exit(0)
              
              latest_change_file = sorted(change_files)[-1]
              print(f"üìä Processing changes from: {latest_change_file}")
              
              # Load changes data
              changes_df = pd.read_csv(latest_change_file)
              
              if changes_df.empty:
                  print("‚ÑπÔ∏è No changes detected, skipping summary creation")
                  exit(0)
              
              # Create summary by change type
              summary_stats = {}
              if 'change_type' in changes_df.columns:
                  summary_stats = changes_df['change_type'].value_counts().to_dict()
              
              # ============ 1. CREATE NEW STOPS ONLY FILE ============
              if 'change_type' in changes_df.columns:
                  new_stops = changes_df[changes_df['change_type'] == 'new'].copy()
                  
                  if not new_stops.empty:
                      # Select and rename columns for better readability
                      if 'corrected_name' in new_stops.columns:
                          new_stops_clean = new_stops[['code', 'corrected_name', 'street', 'lat', 'lon']].copy()
                          new_stops_clean = new_stops_clean.rename(columns={
                              'code': 'Bus Stop Code',
                              'corrected_name': 'Bus Stop Name',
                              'street': 'Street/Road',
                              'lat': 'Latitude',
                              'lon': 'Longitude'
                          })
                      else:
                          new_stops_clean = new_stops[['code', 'name', 'street', 'lat', 'lon']].copy()
                          new_stops_clean = new_stops_clean.rename(columns={
                              'code': 'Bus Stop Code',
                              'name': 'Bus Stop Name', 
                              'street': 'Street/Road',
                              'lat': 'Latitude',
                              'lon': 'Longitude'
                          })
                      
                      # Add metadata columns
                      new_stops_clean.insert(0, 'Change Type', 'NEW')
                      new_stops_clean.insert(1, 'Date Added', readable_date.replace('_', '/'))
                      
                      # Save new stops only file
                      new_stops_file = f"data/new_bus_stops_{readable_date}.csv"
                      new_stops_clean.to_csv(new_stops_file, index=False)
                      
                      print(f"‚úÖ Created new stops file: {new_stops_file} ({len(new_stops_clean)} stops)")
              
              # ============ 2. CREATE NAME CHANGES ONLY FILE ============
              if 'change_type' in changes_df.columns:
                  name_changes = changes_df[changes_df['change_type'] == 'name_changed'].copy()
                  
                  if not name_changes.empty:
                      # Create name changes summary
                      name_changes_clean = name_changes[['code', 'name', 'old_name', 'street']].copy() if 'old_name' in name_changes.columns else name_changes[['code', 'name', 'street']].copy()
                      name_changes_clean = name_changes_clean.rename(columns={
                          'code': 'Bus Stop Code',
                          'name': 'New Name',
                          'old_name': 'Previous Name',
                          'street': 'Street/Road'
                      })
                      
                      # Add metadata
                      name_changes_clean.insert(0, 'Change Type', 'NAME_CHANGED')
                      name_changes_clean.insert(1, 'Date Changed', readable_date.replace('_', '/'))
                      
                      # Save name changes file
                      name_changes_file = f"data/name_changes_{readable_date}.csv"
                      name_changes_clean.to_csv(name_changes_file, index=False)
                      
                      print(f"‚úÖ Created name changes file: {name_changes_file} ({len(name_changes_clean)} changes)")
              
              # ============ 3. CREATE COMPREHENSIVE SUMMARY FILE ============
              # This will be the main "changes only" download
              summary_data = []
              
              # Process each change type
              for change_type in ['new', 'name_changed', 'removed']:
                  if 'change_type' in changes_df.columns:
                      subset = changes_df[changes_df['change_type'] == change_type].copy()
                      
                      if not subset.empty:
                          for _, row in subset.iterrows():
                              summary_row = {
                                  'Change Type': change_type.upper().replace('_', ' '),
                                  'Bus Stop Code': row.get('code', ''),
                                  'Bus Stop Name': row.get('corrected_name', row.get('name', '')),
                                  'Street/Road': row.get('street', ''),
                                  'Latitude': row.get('lat', ''),
                                  'Longitude': row.get('lon', ''),
                                  'Date Processed': readable_date.replace('_', '/'),
                                  'Source': row.get('name_source', 'LTA'),
                              }
                              
                              # Add old name for name changes
                              if change_type == 'name_changed' and 'old_name' in row:
                                  summary_row['Previous Name'] = row.get('old_name', '')
                              
                              # Add change reason if available
                              if 'change_reason' in row:
                                  summary_row['Reason'] = row.get('change_reason', '')
                              
                              summary_data.append(summary_row)
              
              if summary_data:
                  summary_df = pd.DataFrame(summary_data)
                  
                  # Save comprehensive summary
                  summary_file = f"data/bus_stop_changes_summary_{readable_date}.csv"
                  summary_df.to_csv(summary_file, index=False)
                  
                  print(f"‚úÖ Created comprehensive summary: {summary_file} ({len(summary_df)} total changes)")
                  
                  # Create a simple text summary too
                  text_summary = f"""Bus Stop Changes Summary - {readable_date.replace('_', '/')}
          ===============================================

          Total Changes Detected: {len(summary_df)}

          Breakdown by Change Type:
          """
                  
                  for change_type, count in summary_stats.items():
                      text_summary += f"  ‚Ä¢ {change_type.upper().replace('_', ' ')}: {count}\n"
                  
                  text_summary += f"""
          Files Generated:
          ‚Ä¢ bus_stop_changes_summary_{readable_date}.csv - All changes in one file  
          ‚Ä¢ new_bus_stops_{readable_date}.csv - New stops only
          ‚Ä¢ name_changes_{readable_date}.csv - Name changes only

          Data Source: LTA DataMall + SimplyGo corrections
          Generated: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
          """
                  
                  # Save text summary
                  text_summary_file = f"data/changes_summary_{readable_date}.txt"
                  with open(text_summary_file, 'w', encoding='utf-8') as f:
                      f.write(text_summary)
                  
                  print(f"‚úÖ Created text summary: {text_summary_file}")
              
              print("‚úÖ All user-friendly summary files created successfully!")
              
          except Exception as e:
              print(f"‚ùå Error creating summary files: {e}")
              import traceback
              traceback.print_exc()
          EOF

      - name: Check for significant changes
        run: |
          echo "Checking for significant changes..."

          SIGNIFICANT_CHANGE=false

          if [ "${CHANGES_DETECTED:-0}" -gt 50 ]; then
            echo "SIGNIFICANT: Large number of changes detected ($CHANGES_DETECTED)"
            SIGNIFICANT_CHANGE=true
          fi

          if [ "${NEW_STOPS:-0}" -gt 20 ]; then
            echo "SIGNIFICANT: Many new bus stops ($NEW_STOPS)"
            SIGNIFICANT_CHANGE=true
          fi

          if [ "${REMOVED_STOPS:-0}" -gt 20 ]; then
            echo "SIGNIFICANT: Many removed bus stops ($REMOVED_STOPS)"
            SIGNIFICANT_CHANGE=true
          fi

          echo "SIGNIFICANT_CHANGE=$SIGNIFICANT_CHANGE" >> $GITHUB_ENV

      - name: Commit results with summary files
        run: |
          echo "Committing results and summary files to repository..."
          git config --global user.name 'GitHub Actions'
          git config --global user.email 'actions@github.com'

          TIMESTAMP=$(date +'%d%m%Y_%H%M')
          readable_date=$(date +'%d/%m/%Y')

          # Add main data files
          [ -f "data/lta_correction.csv" ] && git add "data/lta_correction.csv"

          # Add all data files
          for file in data/*.csv data/*.txt logs/*.log output/*.csv; do
            [ -f "$file" ] && git add "$file"
          done

          # Count summary files for commit message
          SUMMARY_FILES_COUNT=0
          [ -f "data/new_bus_stops_"*".csv" ] && SUMMARY_FILES_COUNT=$((SUMMARY_FILES_COUNT + 1))
          [ -f "data/name_changes_"*".csv" ] && SUMMARY_FILES_COUNT=$((SUMMARY_FILES_COUNT + 1))  
          [ -f "data/bus_stop_changes_summary_"*".csv" ] && SUMMARY_FILES_COUNT=$((SUMMARY_FILES_COUNT + 1))
          [ -f "data/changes_summary_"*".txt" ] && SUMMARY_FILES_COUNT=$((SUMMARY_FILES_COUNT + 1))

          # Create comprehensive commit message
          COMMIT_MSG="üöå Bus data update ${readable_date}: ${TOTAL_STOPS:-0} stops"

          if [ "${CHANGES_DETECTED:-0}" -gt 0 ]; then
            COMMIT_MSG="$COMMIT_MSG, ${CHANGES_DETECTED} changes detected"
            
            # Add change breakdown
            [ "${NEW_STOPS:-0}" -gt 0 ] && COMMIT_MSG="$COMMIT_MSG (+${NEW_STOPS} new)"
            [ "${NAME_CHANGES:-0}" -gt 0 ] && COMMIT_MSG="$COMMIT_MSG (~${NAME_CHANGES} renamed)"
            [ "${REMOVED_STOPS:-0}" -gt 0 ] && COMMIT_MSG="$COMMIT_MSG (-${REMOVED_STOPS} removed)"
            
            # Add summary files info
            if [ "$SUMMARY_FILES_COUNT" -gt 0 ]; then
              COMMIT_MSG="$COMMIT_MSG, ${SUMMARY_FILES_COUNT} summary files"
            fi
          else
            COMMIT_MSG="$COMMIT_MSG, no changes detected"
          fi

          COMMIT_MSG="$COMMIT_MSG, ${CORRECTIONS:-0} corrections applied"

          # Add efficiency info
          if [ "${CHANGES_DETECTED:-0}" -gt 0 ]; then
            COMMIT_MSG="$COMMIT_MSG (${EFFICIENCY:-100}% efficiency)"
          fi

          # Commit with descriptive message
          if git diff --staged --quiet; then
            echo "‚ÑπÔ∏è No changes to commit"
          else
            git commit -m "$COMMIT_MSG"
            echo "‚úÖ Committed with message: $COMMIT_MSG"
            
            # Show what was committed
            echo "üìÅ Files committed:"
            git diff --name-only HEAD~1 HEAD | sed 's/^/   ‚Ä¢ /'
          fi

          # Push with retry mechanism
          echo "‚¨ÜÔ∏è Pushing to remote repository..."
          if ! git push origin main; then
            echo "‚ö†Ô∏è First push failed, trying pull and rebase..."
            git pull --rebase origin main
            git push origin main
          fi

          echo "‚úÖ Successfully pushed to repository"

      - name: Debug and create fixed Slack payload
        run: |
          echo "üîç Debugging Slack payload creation..."

          # Debug current environment variables
          echo "=== ENVIRONMENT VARIABLES ==="
          echo "CHANGES_DETECTED: ${CHANGES_DETECTED:-0}"
          echo "NEW_STOPS: ${NEW_STOPS:-0}"
          echo "NAME_CHANGES: ${NAME_CHANGES:-0}"
          echo "REMOVED_STOPS: ${REMOVED_STOPS:-0}"
          echo "TOTAL_STOPS: ${TOTAL_STOPS:-0}"
          echo "CORRECTIONS: ${CORRECTIONS:-0}"
          echo "CORRECTION_PCT: ${CORRECTION_PCT:-0}"
          echo "EFFICIENCY: ${EFFICIENCY:-100}"
          echo "PREVIOUS_DATE: ${PREVIOUS_DATE}"
          echo "CURRENT_DATE: ${CURRENT_DATE}"
          echo "STATUS_COLOR: ${STATUS_COLOR}"
          echo "STATUS_EMOJI: ${STATUS_EMOJI}"
          echo "STATUS_TEXT: ${STATUS_TEXT}"
          echo "=============================="

          # Create simplified and safer JSON payload
          python3 << 'EOF'
          import json
          import os
          from datetime import datetime

          # Get environment variables with safe defaults
          changes = int(os.getenv('CHANGES_DETECTED', '0'))
          new_stops = int(os.getenv('NEW_STOPS', '0'))
          name_changes = int(os.getenv('NAME_CHANGES', '0'))
          removed_stops = int(os.getenv('REMOVED_STOPS', '0'))
          total_stops = int(os.getenv('TOTAL_STOPS', '0'))
          corrections = int(os.getenv('CORRECTIONS', '0'))

          # Handle percentage safely
          try:
              correction_pct = float(os.getenv('CORRECTION_PCT', '0'))
              efficiency = float(os.getenv('EFFICIENCY', '100'))
          except:
              correction_pct = 0.0
              efficiency = 100.0

          previous_date = os.getenv('PREVIOUS_DATE', 'Unknown')
          current_date = os.getenv('CURRENT_DATE', 'Unknown')

          # Determine status
          if changes == 0:
              status_color = "good"
              status_emoji = "‚úÖ"
              status_text = "No Changes"
          elif changes <= 20:
              status_color = "warning"
              status_emoji = "‚ö†Ô∏è"
              status_text = "Minor Changes"
          else:
              status_color = "danger"
              status_emoji = "üö®"
              status_text = "Major Changes"

          # Base repository info
          github_repo = os.getenv('GITHUB_REPOSITORY', 'your-repo/bus-stop-data')
          repo_url = f"https://github.com/{github_repo}/raw/main/data"

          # Get current date for file names
          readable_date = datetime.now().strftime("%d_%m_%Y")

          # Build download buttons array
          buttons = []

          # Add changes file button if changes detected
          if changes > 0:
              # Find changes file
              import glob
              change_files = glob.glob('data/LTA_changes_*.csv')
              if change_files:
                  changes_file = os.path.basename(sorted(change_files)[-1])
                  buttons.append({
                      "type": "button",
                      "text": f"üì• All Changes ({changes})",
                      "url": f"{repo_url}/{changes_file}",
                      "style": "primary"
                  })

          # Add new stops button if new stops exist
          if new_stops > 0:
              new_stops_file = f"new_bus_stops_{readable_date}.csv"
              if os.path.exists(f"data/{new_stops_file}"):
                  buttons.append({
                      "type": "button", 
                      "text": f"üÜï New Stops ({new_stops})",
                      "url": f"{repo_url}/{new_stops_file}",
                      "style": "good"
                  })

          # Add name changes button if name changes exist
          if name_changes > 0:
              name_changes_file = f"name_changes_{readable_date}.csv"
              if os.path.exists(f"data/{name_changes_file}"):
                  buttons.append({
                      "type": "button",
                      "text": f"‚úèÔ∏è Name Changes ({name_changes})",
                      "url": f"{repo_url}/{name_changes_file}",
                      "style": "default"
                  })

          # Add summary button if summary exists
          summary_file = f"bus_stop_changes_summary_{readable_date}.csv"
          if os.path.exists(f"data/{summary_file}"):
              buttons.append({
                  "type": "button",
                  "text": "üìä Summary Report",
                  "url": f"{repo_url}/{summary_file}",
                  "style": "default"
              })

          # Always add complete dataset button
          buttons.append({
              "type": "button",
              "text": f"üìä Complete Dataset ({total_stops})",
              "url": f"{repo_url}/lta_correction.csv",
              "style": "default"
          })

          # Add repository and logs buttons
          buttons.extend([
              {
                  "type": "button",
                  "text": "üìà View Repository",
                  "url": f"https://github.com/{github_repo}",
                  "style": "default"
              },
              {
                  "type": "button", 
                  "text": "üîç View Logs",
                  "url": f"https://github.com/{github_repo}/actions",
                  "style": "default"
              }
          ])

          # Create main attachment
          main_attachment = {
              "color": status_color,
              "title": f"{status_emoji} Collection Results - {status_text}",
              "fields": [
                  {
                      "title": "üìä Summary",
                      "value": f"Compared *{previous_date}* ‚Üí *{current_date}*\\nProcessed *{total_stops:,}* total bus stops",
                      "short": False
                  },
                  {
                      "title": "üìà Changes Detected", 
                      "value": str(changes),
                      "short": True
                  },
                  {
                      "title": "üÜï New Stops",
                      "value": str(new_stops),
                      "short": True
                  },
                  {
                      "title": "‚úèÔ∏è Name Changes",
                      "value": str(name_changes),
                      "short": True
                  },
                  {
                      "title": "üóëÔ∏è Removed Stops", 
                      "value": str(removed_stops),
                      "short": True
                  },
                  {
                      "title": "üîÑ Corrections Applied",
                      "value": f"{corrections} ({correction_pct:.2f}%)",
                      "short": True
                  },
                  {
                      "title": "‚ö° Efficiency",
                      "value": f"{efficiency:.2f}%",
                      "short": True
                  }
              ],
              "actions": buttons,
              "footer": "Bus Stop SG Collection | Auto-updated weekly",
              "ts": int(datetime.now().timestamp())
          }

          # Create attachments array
          attachments = [main_attachment]

          # Add new stops details if available
          if new_stops > 0:
              try:
                  new_stops_details = json.loads(os.getenv('NEW_STOPS_DETAILS', '[]'))
                  if new_stops_details:
                      stops_text = []
                      for stop in new_stops_details[:5]:
                          code = stop.get('code', 'N/A')
                          name = stop.get('name', 'N/A')
                          street = stop.get('street', 'N/A')
                          stops_text.append(f"‚Ä¢ `{code}` - {name} ({street})")
                      
                      details_text = "\\n".join(stops_text)
                      if len(new_stops_details) > 5:
                          details_text += f"\\n‚Ä¢ ... and {len(new_stops_details) - 5} more new stops"
                      
                      attachments.append({
                          "color": "#36a64f",
                          "title": "üÜï New Bus Stops Details",
                          "text": details_text,
                          "footer": f"Click 'New Stops ({new_stops})' button above to download CSV with these stops"
                      })
              except:
                  pass

          # Add download info if changes detected
          if changes > 0:
              download_info = []
              
              if changes > 0:
                  download_info.append("‚Ä¢ **All Changes**: Complete change log with technical details")
              if new_stops > 0:
                  download_info.append(f"‚Ä¢ **New Stops Only**: {new_stops} newly added bus stops")
              if name_changes > 0:
                  download_info.append("‚Ä¢ **Name Changes**: Stops with updated names")
              if os.path.exists(f"data/bus_stop_changes_summary_{readable_date}.csv"):
                  download_info.append("‚Ä¢ **Summary Report**: User-friendly consolidated changes")
              download_info.append("‚Ä¢ **Complete Dataset**: All bus stops with corrections")
              
              attachments.append({
                  "color": "#3498db",
                  "title": "üì• Available Downloads",
                  "text": "\\n".join(download_info),
                  "footer": "All files are automatically generated and ready for download"
              })

          # Create final payload
          payload = {
              "text": "üöå Bus Stop Collection Complete",
              "attachments": attachments
          }

          # Save payload to file
          with open('slack_payload_fixed.json', 'w') as f:
              json.dump(payload, f, indent=2)

          print("‚úÖ Fixed Slack payload created successfully")
          print("üìÑ Payload structure:")
          print(f"   - Main text: {payload['text']}")
          print(f"   - Attachments: {len(payload['attachments'])}")
          print(f"   - Buttons: {len(buttons)}")
          print(f"   - Total size: ~{len(json.dumps(payload))} characters")

          EOF

          echo "‚úÖ Fixed payload generation completed"

      - name: Validate and send fixed Slack notification
        if: success()
        env:
          SLACK_WEBHOOK: ${{ secrets.SLACK_WEBHOOK }}
        run: |
          if [ -n "$SLACK_WEBHOOK" ]; then
            echo "üîç Validating JSON payload..."
            
            # Check if payload file exists and is valid JSON
            if [ -f "slack_payload_fixed.json" ]; then
              echo "üìÑ Payload file found, checking JSON validity..."
              
              # Validate JSON
              if python3 -m json.tool slack_payload_fixed.json > /dev/null 2>&1; then
                echo "‚úÖ JSON is valid"
                
                # Show payload size
                PAYLOAD_SIZE=$(wc -c < slack_payload_fixed.json)
                echo "üìè Payload size: $PAYLOAD_SIZE bytes"
                
                # Send to Slack
                echo "üì§ Sending fixed notification to Slack..."
                
                RESPONSE=$(curl -s -w "\n%{http_code}" -X POST \
                  -H 'Content-type: application/json' \
                  -d @slack_payload_fixed.json \
                  "$SLACK_WEBHOOK")
                
                HTTP_CODE=$(echo "$RESPONSE" | tail -n1)
                RESPONSE_BODY=$(echo "$RESPONSE" | head -n -1)
                
                echo "üìä Response status: $HTTP_CODE"
                echo "üìù Response body: $RESPONSE_BODY"
                
                if [ "$HTTP_CODE" -eq 200 ] && [ "$RESPONSE_BODY" = "ok" ]; then
                  echo "‚úÖ Enhanced notification sent successfully!"
                else
                  echo "‚ùå Failed to send notification"
                  echo "   Status: $HTTP_CODE"
                  echo "   Response: $RESPONSE_BODY"
                  
                  # Show first 500 chars of payload for debugging
                  echo "üîç Payload preview (first 500 chars):"
                  head -c 500 slack_payload_fixed.json
                  echo ""
                fi
              else
                echo "‚ùå Invalid JSON in payload file"
                echo "üîç JSON validation error:"
                python3 -m json.tool slack_payload_fixed.json 2>&1 || true
              fi
            else
              echo "‚ùå Payload file not found"
              ls -la slack_payload*.json || echo "No payload files found"
            fi
          else
            echo "‚ö†Ô∏è SLACK_WEBHOOK not configured, skipping notification"
          fi

      - name: Send failure notification
        if: failure()
        env:
          SLACK_WEBHOOK: ${{ secrets.SLACK_WEBHOOK }}
        run: |
          if [ -n "$SLACK_WEBHOOK" ]; then
            curl -X POST -H 'Content-type: application/json' \
              --data '{
                "text":"‚ùå Bus Stop Collection Failed",
                "attachments":[{
                  "color":"danger",
                  "title":"üö® Enhanced Workflow Error",
                  "text":"The enhanced bus stop collection workflow has failed. Please check the logs for details.",
                  "actions":[{
                    "type":"button",
                    "text":"üîç Check Logs",
                    "url":"https://github.com/'${GITHUB_REPOSITORY}'/actions",
                    "style":"danger"
                  }]
                }]
              }' \
              "$SLACK_WEBHOOK"
          fi

      - name: Summary
        if: always()
        run: |
          echo "================================="
          echo "ENHANCED WORKFLOW COMPLETE"
          echo "================================="
          echo "Total Stops: ${TOTAL_STOPS:-0}"
          echo "Changes Detected: ${CHANGES_DETECTED:-0}"
          echo "  - New: ${NEW_STOPS:-0}"
          echo "  - Renamed: ${NAME_CHANGES:-0}"
          echo "  - Removed: ${REMOVED_STOPS:-0}"
          echo "Corrections: ${CORRECTIONS:-0} (${CORRECTION_PCT:-0}%)"
          echo "Efficiency: ${EFFICIENCY:-100}%"
          echo "Date Range: ${PREVIOUS_DATE} ‚Üí ${CURRENT_DATE}"
          echo "Summary Files Generated: Yes"
          echo "Dynamic Downloads: Available"
          echo "Mode: Enhanced with Smart Downloads"
          echo "================================="
